# ============================================
# 探索→ナビ SLAM PPO
# Episode内ステップ数評価（完成形・1コード）
# ============================================

# ---------- install ----------
!pip install -q stable-baselines3 gymnasium minigrid matplotlib numpy

# ---------- import ----------
import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt

from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback
from minigrid.wrappers import RGBImgObsWrapper, ImgObsWrapper

# ============================================
# MemoryMap Wrapper（簡易 SLAM）
# ============================================
class MemoryMapWrapper(gym.Wrapper):
    def __init__(self, env):
        super().__init__(env)
        h, w = env.unwrapped.height, env.unwrapped.width
        self.memory_map = np.zeros((h, w), dtype=np.float32)

    def reset(self, **kwargs):
        obs, info = self.env.reset(**kwargs)
        self.memory_map[:] = 0
        self._update()
        return obs, info

    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)
        self._update()
        return obs, reward, terminated, truncated, info

    def _update(self):
        x, y = self.env.unwrapped.agent_pos
        self.memory_map[x, y] = 1.0


# ============================================
# SLAM 観測（画像 + MemoryMap）
# ============================================
class SLAMObsWrapper(gym.ObservationWrapper):
    def __init__(self, env):
        super().__init__(env)
        h, w = env.memory_map.shape

        self.observation_space = gym.spaces.Dict({
            "image": env.observation_space,
            "memory": gym.spaces.Box(
                low=0, high=1,
                shape=(h, w),
                dtype=np.float32
            )
        })

    def observation(self, obs):
        return {
            "image": obs,
            "memory": self.env.memory_map.copy()
        }


# ============================================
# 探索→ナビ 報酬切替 Wrapper
# ============================================
class ExploreToNavigateReward(gym.Wrapper):
    def __init__(self, env, explore_bonus=0.02, goal_reward=1.0):
        super().__init__(env)
        self.explore_bonus = explore_bonus
        self.goal_reward = goal_reward
        self.prev_covered = 0

    def reset(self, **kwargs):
        obs, info = self.env.reset(**kwargs)
        self.prev_covered = 0
        return obs, info

    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)

        # ★ 修正ポイント
        memory_map = self.env.get_wrapper_attr("memory_map")
        covered = np.sum(memory_map)

        reward += max(0, covered - self.prev_covered) * self.explore_bonus
        self.prev_covered = covered

        if terminated:
            reward += self.goal_reward
            info["is_success"] = True

        return obs, reward, terminated, truncated, info

# ============================================
# Episode内ステップ数評価 Callback（完成形）
# ============================================
class EpisodeStepEvalCallback(BaseCallback):
    def __init__(self):
        super().__init__()
        self.goal_steps = []
        self.episode_step = 0

    def _on_step(self) -> bool:
        self.episode_step += 1

        infos = self.locals["infos"]
        dones = self.locals["dones"]

        for i, done in enumerate(dones):
            if done:
                if infos[i].get("is_success", False):
                    self.goal_steps.append(self.episode_step)
                self.episode_step = 0
        return True

    def plot(self, window=30):
        steps = np.array(self.goal_steps)

        plt.figure(figsize=(8, 5))
        plt.plot(steps, alpha=0.3, label="Raw")

        if len(steps) >= window:
            ma = np.convolve(
                steps,
                np.ones(window)/window,
                mode="valid"
            )
            plt.plot(ma, linewidth=2, label=f"Moving Avg ({window})")

        plt.xlabel("Goal Episode")
        plt.ylabel("Steps to Goal")
        plt.title("探索→ナビ SLAM PPO\nEpisode内ゴール到達ステップ数")
        plt.grid()
        plt.legend()
        plt.show()


# ============================================
# 環境構築（順番重要）
# ============================================
env = gym.make("MiniGrid-Empty-8x8-v0")
env = RGBImgObsWrapper(env)
env = ImgObsWrapper(env)
env = MemoryMapWrapper(env)
env = SLAMObsWrapper(env)
env = ExploreToNavigateReward(env)

# ============================================
# PPO（MultiInput）
# ============================================
model = PPO(
    "MultiInputPolicy",
    env,
    n_steps=256,
    batch_size=64,
    learning_rate=3e-4,
    verbose=1
)

# ============================================
# 学習
# ============================================
callback = EpisodeStepEvalCallback()

model.learn(
    total_timesteps=10_000,
    callback=callback
)

# ============================================
# 評価プロット
# ============================================
callback.plot(window=20)