# ===========================================
# Block Breaker (Breakout-like) - PPO Training + Video (Colab-ready)
# Specifications: follow user's detailed scenario (PPO, curriculum, videos)
# Paste this single cell into Google Colab and run.
# ===========================================

# install ffmpeg for video output
!apt-get -y install ffmpeg > /dev/null

# basic imports
import math
import random
import time
from collections import deque, namedtuple

import numpy as np
import cv2
import imageio.v2 as imageio
from IPython.display import Video, display
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.optim as optim

# -----------------------------
# Environment: BlockBreakEnv
# Observation (fixed-length):
# [racket_x, ball_x, ball_y, ball_vx, ball_vy, flags(16)] => 21 dims
# Actions: 5 discrete: left_large(-6), left_small(-2), stop(0), right_small(+2), right_large(+6)
# -----------------------------
class BlockBreakEnv:
    def __init__(self, level=1, max_blocks=16):
        self.level = level
        self.max_blocks = max_blocks
        self.h = 200
        self.w = 240
        self.reset_level_settings()

    def reset_level_settings(self):
        # set court by level
        if self.level == 1:
            self.court_min_x = 50
            self.court_max_x = 170
            self.block_list = [
                (60,150,109,159),(110,150,159,159),(60,160,109,169),(110,160,159,169)
            ]
        elif self.level == 2:
            self.court_min_x = 25
            self.court_max_x = 195
            self.block_list = [
                (35,150,84,159),(85,150,134,159),(135,150,184,159),
                (35,160,84,169),(85,160,134,169),(135,160,184,169),
                (35,170,84,179),(85,170,134,179),(135,170,184,179)
            ]
        else:
            self.court_min_x = 0
            self.court_max_x = 220
            xs = [10,60,110,160]
            ys = [150,160,170,180]
            self.block_list = []
            for y in ys:
                for x in xs:
                    self.block_list.append((x,y,x+49,y+9))

        # pad to max_blocks
        self.initial_blocks = list(self.block_list)
        # lives: ball can drop up to 3 times, 4th drop ends episode
        self.lives = 4
        self.reset()
        return self._observe()

    def reset(self):
        # reset dynamic state
        self.blocks = list(self.initial_blocks)
        self.racket_x = 75.0
        self.racket_y = 10.0
        self.racket_width = 50
        self.racket_hits = 0
        self.ball_x = self.racket_x + self.racket_width/2.0
        self.ball_y = 20.0
        # speed vector magnitude must be approx 3 px/st
        angle = random.uniform(math.radians(30), math.radians(150))
        vx = 3.0 * math.cos(angle)
        vy = 3.0 * math.sin(angle)
        self.ball_vx = vx
        self.ball_vy = vy
        self.done = False
        self.steps = 0
        self.max_steps = 5000
        return self._observe()

    def seed(self, s):
        random.seed(s); np.random.seed(s)

    def _observe(self):
        # fixed-length flags only (no block coords)
        flags = [0]*self.max_blocks
        for i in range(min(len(self.blocks), self.max_blocks)):
            flags[i] = 1
        obs = [self.racket_x, self.ball_x, self.ball_y, self.ball_vx, self.ball_vy] + flags
        return np.array(obs, dtype=np.float32)

    def _is_ball_in_racket_zone(self):
        return (self.racket_x < self.ball_x) and (self.ball_x < self.racket_x + 40)

    def step(self, action):
        # action mapping
        act_map = {0:-6.0, 1:-2.0, 2:0.0, 3:2.0, 4:6.0}
        move = act_map.get(int(action), 0.0)
        self.racket_x += move
        # clamp racket inside court
        self.racket_x = float(np.clip(self.racket_x, self.court_min_x, self.court_max_x - self.racket_width))

        # update ball
        self.ball_x += self.ball_vx
        self.ball_y += self.ball_vy
        self.steps += 1

        reward = 0.0
        info = {}
        done = False

        # wall reflection (left/right/top)
        if self.ball_x <= self.court_min_x:
            self.ball_x = self.court_min_x
            self.ball_vx *= -1
        if self.ball_x >= self.court_max_x:
            self.ball_x = self.court_max_x
            self.ball_vx *= -1
        if self.ball_y >= self.h:
            self.ball_y = self.h
            self.ball_vy *= -1

        # racket collision: racket is rectangle at y=10..20, x=racket_x..racket_x+50
        if (self.racket_y <= self.ball_y <= self.racket_y + 10) and (self.racket_x <= self.ball_x <= self.racket_x + self.racket_width):
            # reflect vertical velocity
            self.ball_vy *= -1
            self.racket_hits += 1
            reward += 0.0  # base: other bonuses applied below
            # reflection angle change depending where it hit
            relative = (self.ball_x - self.racket_x) / self.racket_width
            if relative < 0.25:
                # add 20 degrees to angle (approx)
                # rotate vector by +20 degrees
                ang = math.atan2(self.ball_vy, self.ball_vx)
                ang += math.radians(20)
                mag = math.hypot(self.ball_vx, self.ball_vy)
                self.ball_vx = mag * math.cos(ang)
                self.ball_vy = mag * math.sin(ang)
            elif relative > 0.75:
                ang = math.atan2(self.ball_vy, self.ball_vx)
                ang -= math.radians(20)
                mag = math.hypot(self.ball_vx, self.ball_vy)
                self.ball_vx = mag * math.cos(ang)
                self.ball_vy = mag * math.sin(ang)

        # block collisions: if ball inside block rect -> remove block and reflect
        removed = 0
        new_blocks = []
        for (x1,y1,x2,y2) in self.blocks:
            if (x1 <= self.ball_x <= x2) and (y1 <= self.ball_y <= y2):
                removed += 1
                # reflect vertically for simplicity
                self.ball_vy *= -1
            else:
                new_blocks.append((x1,y1,x2,y2))
        if removed > 0:
            reward += 3.0 * removed  # +3 per block
        self.blocks = new_blocks

        # Rx<Bx<Rx+40 bonus/penalty
        if self._is_ball_in_racket_zone():
            reward += 3.0
        else:
            reward -= 0.5

        # ball drop (y <= 0). In this scenario, ball falling below y=0 means a drop occurs but episode only ends on 4th drop
        if self.ball_y <= 0:
            # penalize -1 and reset ball to starting on racket
            reward -= 1.0
            self.lives -= 1
            if self.lives <= 0:
                done = True
            else:
                # reset ball onto racket
                self.ball_x = self.racket_x + self.racket_width/2.0
                self.ball_y = 20.0
                # random launch direction
                ang = random.uniform(math.radians(30), math.radians(150))
                mag = 3.0
                self.ball_vx = mag * math.cos(ang)
                self.ball_vy = mag * math.sin(ang)

        # success conditions
        success = False
        if len(self.blocks) == 0:
            reward += 20.0
            done = True
            success = True
        if self.racket_hits >= 5:
            reward += 20.0
            done = True
            success = True

        # step limit
        if self.steps >= self.max_steps:
            done = True

        info['racket_x'] = self.racket_x
        info['ball_x'] = self.ball_x
        info['ball_y'] = self.ball_y
        info['blocks'] = list(self.blocks)
        info['court_min_x'] = self.court_min_x
        info['court_max_x'] = self.court_max_x
        info['success'] = success
        info['lives'] = self.lives

        return self._observe(), reward, done, info

    def render(self, W=240, H=240):
        img = np.zeros((H, W, 3), dtype=np.uint8)
        # court box
        cv2.rectangle(img, (int(self.court_min_x), 0), (int(self.court_max_x), 200), (100,100,100), 2)
        # racket
        cv2.rectangle(img, (int(self.racket_x), 10), (int(self.racket_x)+self.racket_width, 20), (255,255,255), -1)
        # ball
        cv2.circle(img, (int(self.ball_x), int(self.ball_y)), 5, (0,255,0), -1)
        # blocks
        for (x1,y1,x2,y2) in self.blocks:
            cv2.rectangle(img, (int(x1),int(y1)), (int(x2),int(y2)), (255,0,0), -1)
        return img

# -----------------------------
# PPO implementation (simple)
# -----------------------------
class ActorCritic(nn.Module):
    def __init__(self, obs_dim, act_dim):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(obs_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU()
        )
        self.policy = nn.Linear(128, act_dim)
        self.value = nn.Linear(128, 1)

    def forward(self, x):
        x = self.fc(x)
        return self.policy(x), self.value(x).squeeze(-1)

# helper: compute returns and advantages (GAE)

def compute_gae(rewards, masks, values, next_value, gamma=0.99, lam=0.95):
    values = values + [next_value]
    gae = 0
    returns = []
    for step in reversed(range(len(rewards))):
        delta = rewards[step] + gamma * values[step+1] * masks[step] - values[step]
        gae = delta + gamma * lam * masks[step] * gae
        returns.insert(0, gae + values[step])
    advs = [ret - val for ret, val in zip(returns, values[:-1])]
    return returns, advs

# -----------------------------
# utilities: video, plotting
# -----------------------------
def create_video(frames, filepath='out.mp4', fps=30):
    if len(frames) == 0:
        print('No frames to save')
        return
    imageio.mimsave(filepath, frames, fps=fps)
    print('Saved video:', filepath)

def show_video(filepath):
    display(Video(filepath, embed=True))

# -----------------------------
# Training loop with curriculum
# -----------------------------

def train_ppo(num_levels=3, max_episodes=1000, ppo_epochs=4, mini_batch_size=64, total_timesteps=200000):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print('Device:', device)

    level = 1
    global_step = 0

    # logging
    level_videos = []
    episode_rewards = []
    episode_success = deque(maxlen=10)

    while level <= num_levels:
        print(f'=== Start Level {level} ===')
        env = BlockBreakEnv(level=level)
        obs_dim = len(env._observe())
        act_dim = 5

        model = ActorCritic(obs_dim, act_dim).to(device)
        optimizer = optim.Adam(model.parameters(), lr=3e-4)

        # training buffers
        all_episode_rewards = []
        recent_success = deque(maxlen=10)

        episode = 0
        while True:
            # collect rollout
            obs_buf = []
            actions_buf = []
            logp_buf = []
            rewards_buf = []
            masks_buf = []
            values_buf = []

            obs = env.reset()
            ep_reward = 0.0
            done = False
            steps = 0
            success_flag = False

            while not done and steps < 5000:
                steps += 1
                global_step += 1
                obs_tensor = torch.tensor(obs, dtype=torch.float32).to(device)
                logits, value = model(obs_tensor)
                probs = torch.softmax(logits, dim=-1)
                dist = torch.distributions.Categorical(probs)
                action = dist.sample().item()
                logp = dist.log_prob(torch.tensor(action).to(device)).item()

                next_obs, reward, done, info = env.step(action)

                obs_buf.append(obs)
                actions_buf.append(action)
                logp_buf.append(logp)
                rewards_buf.append(reward)
                masks_buf.append(0.0 if done else 1.0)
                values_buf.append(value.item())

                obs = next_obs
                ep_reward += reward

                if info.get('success', False):
                    success_flag = True

            # end episode
            episode += 1
            all_episode_rewards.append(ep_reward)
            recent_success.append(1.0 if success_flag else 0.0)
            episode_success.append(1.0 if success_flag else 0.0)
            episode_rewards.append(ep_reward)

            # compute advantage and returns
            # get last value (bootstrap)
            with torch.no_grad():
                last_obs = torch.tensor(obs, dtype=torch.float32).to(device)
                _, last_val = model(last_obs)
                next_value = last_val.item()
            returns, advs = compute_gae(rewards_buf, masks_buf, values_buf, next_value)

            # convert to tensors
            obs_batch = torch.tensor(np.array(obs_buf, dtype=np.float32)).to(device)
            act_batch = torch.tensor(np.array(actions_buf, dtype=np.int64)).to(device)
            old_logp_batch = torch.tensor(np.array(logp_buf, dtype=np.float32)).to(device)
            return_batch = torch.tensor(np.array(returns, dtype=np.float32)).to(device)
            adv_batch = torch.tensor(np.array(advs, dtype=np.float32)).to(device)
            adv_batch = (adv_batch - adv_batch.mean()) / (adv_batch.std() + 1e-8)

            # PPO update
            for _ in range(ppo_epochs):
                # for simplicity sample minibatches randomly
                idxs = np.arange(len(obs_batch))
                np.random.shuffle(idxs)
                for start in range(0, len(obs_batch), mini_batch_size):
                    mb_idx = idxs[start:start+mini_batch_size]
                    mb_obs = obs_batch[mb_idx]
                    mb_acts = act_batch[mb_idx]
                    mb_old_logp = old_logp_batch[mb_idx]
                    mb_ret = return_batch[mb_idx]
                    mb_adv = adv_batch[mb_idx]

                    logits, vals = model(mb_obs)
                    probs = torch.softmax(logits, dim=-1)
                    dist = torch.distributions.Categorical(probs)
                    new_logp = dist.log_prob(mb_acts)
                    entropy = dist.entropy().mean()

                    ratio = torch.exp(new_logp - mb_old_logp)
                    surr1 = ratio * mb_adv
                    surr2 = torch.clamp(ratio, 1.0-0.2, 1.0+0.2) * mb_adv
                    policy_loss = -torch.min(surr1, surr2).mean()
                    value_loss = nn.functional.mse_loss(vals, mb_ret)
                    loss = policy_loss + 0.5 * value_loss - 0.01 * entropy

                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()

            # logging
            avg_recent_reward = np.mean(all_episode_rewards[-20:]) if len(all_episode_rewards)>=1 else ep_reward
            succ_rate_10 = np.mean(list(recent_success)) if len(recent_success)>0 else 0.0
            print(f'Level {level} Ep {episode} reward={ep_reward:.2f} recent10_success={succ_rate_10:.2f}')

            # curriculum check: after at least 10 episodes, if success_rate >=0.8 on recent 10 episodes -> save video and advance
            if len(recent_success) == 10 and np.mean(recent_success) >= 0.8:
                print(f'Level {level} success rate >= 0.8; saving level video and advancing')
                # save level video
                video_path = f'level_{level}_policy.mp4'
                eval_and_save_video(env, model, video_path)
                level_videos.append(video_path)
                level += 1
                break

            # safety stop conditions
            if episode >= max_episodes:
                print('Reached max episodes for this level')
                break

        # end level while
    # end curriculum
    print('Training finished for all levels')
    # final evaluation: save final video on last level env
    final_env = BlockBreakEnv(level=num_levels)
    final_video = 'final_trained_policy.mp4'
    eval_and_save_video(final_env, model, final_video)

    # plot reward curve
    plt.figure(figsize=(10,4))
    plt.plot(episode_rewards)
    plt.xlabel('Episode')
    plt.ylabel('Total Reward')
    plt.title('Episode reward over time')
    plt.grid(True)
    plt.show()

    return level_videos, final_video

# evaluation helper
def eval_and_save_video(env, model, save_path, episodes=1, fps=30):
    model_cpu = model.to('cpu')
    frames = []
    for ep in range(episodes):
        obs = env.reset()
        done = False
        while not done:
            obs_t = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)
            logits, _ = model_cpu(obs_t)
            probs = torch.softmax(logits, dim=-1).detach().numpy()[0]
            action = int(np.argmax(probs))
            obs, r, done, info = env.step(action)
            frames.append(env.render())
            if len(frames) > 6000:
                break
    # ensure frames are RGB images
    frames_rgb = [cv2.cvtColor(f, cv2.COLOR_BGR2RGB) for f in frames]
    create_video(frames_rgb, save_path, fps=fps)
    show_video(save_path)

# -----------------------------
# Run training
# -----------------------------
# NOTE: Adjust max_episodes and total_timesteps as desired for Colab runtime
level_videos, final_video = train_ppo(num_levels=3, max_episodes=200, ppo_epochs=4, mini_batch_size=64)
print('Videos saved:', level_videos, final_video)

# show final video if exists
try:
    show_video(final_video)
except Exception:
    pass

# End of notebook cell
