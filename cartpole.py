# ===============================
# ğŸ“¦ Step 1: ãƒ©ã‚¤ãƒ–ãƒ©ãƒªæº–å‚™
# ===============================
!pip install gymnasium[classic_control] torch matplotlib tqdm imageio -q

import gymnasium as gym
import torch
import torch.nn as nn
import torch.optim as optim
import random
import numpy as np
from collections import deque
import matplotlib.pyplot as plt
from tqdm import trange
import imageio
from IPython.display import Image

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# ===============================
# ğŸ§  Step 2: DQNãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å®šç¾©
# ===============================
class DQN(nn.Module):
    def __init__(self, state_size, action_size):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_size, 128)
        self.fc2 = nn.Linear(128, 128)
        self.out = nn.Linear(128, action_size)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.out(x)

# ===============================
# âš™ï¸ Step 3: ç’°å¢ƒã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
# ===============================
env = gym.make("CartPole-v1", render_mode=None)
state_size = env.observation_space.shape[0]
action_size = env.action_space.n

policy_net = DQN(state_size, action_size).to(device)
target_net = DQN(state_size, action_size).to(device)
target_net.load_state_dict(policy_net.state_dict())
target_net.eval()

optimizer = optim.Adam(policy_net.parameters(), lr=1e-4)
replay_buffer = deque(maxlen=50000)
batch_size = 128
gamma = 0.99
epsilon = 1.0
epsilon_min = 0.05
epsilon_decay = 0.995
target_update_freq = 50
num_episodes = 1000

rewards_log = []
losses = []

# ===============================
# ğŸš€ Step 4: ãƒ¡ã‚¤ãƒ³å­¦ç¿’ãƒ«ãƒ¼ãƒ—
# ===============================
for episode in trange(num_episodes):
    state, _ = env.reset()
    state = torch.FloatTensor(state).unsqueeze(0).to(device)
    total_reward = 0
    done = False

    while not done:
        # Îµ-greedy è¡Œå‹•é¸æŠ
        if random.random() < epsilon:
            action = env.action_space.sample()
        else:
            with torch.no_grad():
                q_values = policy_net(state)
                action = torch.argmax(q_values).item()

        next_state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        next_state = torch.FloatTensor(next_state).unsqueeze(0).to(device)

        # ğŸ¯ æ»‘ã‚‰ã‹å ±é…¬è£œæ­£ï¼ˆè§’åº¦ï¼†ä½ç½®ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼‰
        angle_penalty = abs(next_state[0, 2].item()) * 2.0
        position_penalty = abs(next_state[0, 0].item()) * 0.5
        reward = reward - angle_penalty - position_penalty

        # å€’ã‚ŒãŸã¨ãã®è»½ã„ãƒšãƒŠãƒ«ãƒ†ã‚£
        if done and total_reward < 195:
            reward -= 5.0

        # çµŒé¨“ã‚’ãƒªãƒ—ãƒ¬ã‚¤ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
        replay_buffer.append((state, action, reward, next_state, done))
        state = next_state
        total_reward += reward

        # å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—
        if len(replay_buffer) >= batch_size:
            batch = random.sample(replay_buffer, batch_size)
            states, actions, rewards, next_states, dones = zip(*batch)

            states = torch.cat(states).to(device)
            actions = torch.LongTensor(actions).unsqueeze(1).to(device)
            rewards = torch.FloatTensor(rewards).to(device)
            next_states = torch.cat(next_states).to(device)
            dones = torch.BoolTensor(dones).to(device)

            q_values = policy_net(states).gather(1, actions).squeeze(1)
            with torch.no_grad():
                next_q = target_net(next_states).max(1)[0]
                target = rewards + gamma * next_q * (~dones)

            loss = nn.functional.smooth_l1_loss(q_values, target)
            optimizer.zero_grad()
            loss.backward()
            nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)
            optimizer.step()
            losses.append(loss.item())

    # Îµæ¸›è¡°
    if epsilon > epsilon_min:
        epsilon *= epsilon_decay

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆæ›´æ–°
    if episode % target_update_freq == 0:
        target_net.load_state_dict(policy_net.state_dict())

    rewards_log.append(total_reward)

env.close()

# ===============================
# ğŸ“ˆ Step 5: ã‚°ãƒ©ãƒ•æç”»ï¼ˆæ»‘ã‚‰ã‹ç·šä»˜ãï¼‰
# ===============================
window = 20
smoothed = np.convolve(rewards_log, np.ones(window)/window, mode='valid')

plt.figure(figsize=(10,5))
plt.plot(rewards_log, alpha=0.3, label="raw")
plt.plot(smoothed, label=f"{window}-ep moving avg", linewidth=2)
plt.xlabel("Episode")
plt.ylabel("Total Reward")
plt.title("DQN (CartPole-v1) with Smooth Reward Function")
plt.legend()
plt.show()

# ===============================
# ğŸ¬ Step 6: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§å‹•ç”»ç”Ÿæˆ
# ===============================
env = gym.make("CartPole-v1", render_mode="rgb_array")
frames = []
state, _ = env.reset()
done = False
total_reward = 0

while not done:
    frame = env.render()
    frames.append(frame)

    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
    with torch.no_grad():
        q_values = policy_net(state_tensor)
        action = torch.argmax(q_values).item()

    state, reward, terminated, truncated, _ = env.step(action)
    done = terminated or truncated
    total_reward += reward

env.close()
print(f"ğŸ‰ å†ç”Ÿã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ã‚¹ã‚³ã‚¢: {total_reward:.1f}")

# mp4ã«ä¿å­˜
imageio.mimsave("cartpole_result.mp4", frames, fps=30)
print("ğŸ¥ å‹•ç”»ã‚’ 'cartpole_result.mp4' ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸã€‚")

# å‹•ç”»ã‚’Colabä¸Šã§å†ç”Ÿ
from IPython.display import Video

# Colabä¸Šã§å‹•ç”»ã‚’å†ç”Ÿ
Video("cartpole_result.mp4", embed=True)