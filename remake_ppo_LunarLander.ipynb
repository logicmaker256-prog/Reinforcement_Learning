# ====================================================
# Colabç”¨ï¼šLunarLander (ã‚«ã‚¹ã‚¿ãƒ å ±é…¬ï¼æ——ä¸­å¤®å¿—å‘) + PPO å­¦ç¿’ + å‹•ç”»ç”Ÿæˆ/å†ç”Ÿ
# ã‚³ãƒ”ãƒ¼ï¼†ãƒšãƒ¼ã‚¹ãƒˆã§å‹•ã1ã‚»ãƒ«å®Œçµç‰ˆ
# ====================================================

# ---- å¿…è¦ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆColabå‘ã‘ï¼‰ ----
!apt-get update -qq
!apt-get install -qq swig
!pip install -q gymnasium[box2d]
!pip install -q stable-baselines3
!pip install -q imageio[ffmpeg]

# ---- ãƒ©ã‚¤ãƒ–ãƒ©ãƒªèª­ã¿è¾¼ã¿ ----
import os
import gymnasium as gym
import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from gymnasium.wrappers import RecordVideo
from IPython.display import Video, display
import time

# -------------------------
# ã‚«ã‚¹ã‚¿ãƒ å ±é…¬ãƒ©ãƒƒãƒ‘ãƒ¼
# -------------------------
class CenterRewardWrapper(gym.Wrapper):
    """
    åŸºæœ¬ã‚¢ã‚¤ãƒ‡ã‚¢ï¼š
      - å„ step ã® reward ã«å¯¾ã—ã¦æ¨ªãšã‚Œï¼ˆabs(x)ï¼‰ã«æ¯”ä¾‹ã—ãŸç½°ã‚’è¿½åŠ 
      - ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†ï¼ˆç€åœ°æˆåŠŸï¼‰æ™‚ã«ä½ç½®ã«å¿œã˜ãŸå¤§ããªãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆ/ å ±é…¬èª¿æ•´ï¼‰ã‚’è¿½åŠ 
    observation (LunarLander) ã®æ§‹æˆã¯:
      [0]=pos.x, [1]=pos.y, [2]=vel.x, [3]=vel.y, [4]=angle, [5]=angularVel, [6]=leftLeg, [7]=rightLeg
    """
    def __init__(self, env, step_penalty_weight=2.0, landing_penalty_weight=50.0):
        super().__init__(env)
        self.step_penalty_weight = step_penalty_weight
        self.landing_penalty_weight = landing_penalty_weight

    def step(self, action):
        obs, base_reward, terminated, truncated, info = self.env.step(action)
        # æ¨ªæ–¹å‘ã®ãšã‚Œ
        try:
            x = float(obs[0])
        except Exception:
            x = 0.0

        # 1) å„ step ã«å¯¾ã™ã‚‹å°ã•ãªä½ç½®ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆä¸­å¿ƒã¸èª˜å°ï¼‰
        shaped_reward = base_reward - abs(x) * self.step_penalty_weight

        # 2) ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†ï¼ˆæˆåŠŸç€åœ° or crashï¼‰ã®ã¨ãã€ç€åœ°ã®è‰¯ã—æ‚ªã—ã«å¿œã˜ã¦è¿½åŠ èª¿æ•´
        #    ã“ã“ã§ã¯ base_reward>0 ã‚’ã€ŒæˆåŠŸç€åœ°ã€ã®ç°¡æ˜“åˆ¤å®šã¨ã™ã‚‹ï¼ˆå…ƒä»•æ§˜ã§ã¯ç€åœ°æˆåŠŸã§ +100 ãªã©ï¼‰
        if (terminated or truncated):
            if base_reward > 0:  # æˆåŠŸç€åœ°ã£ã½ã„å ´åˆ
                # æˆåŠŸæ™‚ã«ä½ç½®ãŒä¸­å¤®ã‹ã‚‰é›¢ã‚Œã¦ã„ã‚‹ã¨å¤§ããæ¸›ç‚¹ï¼ˆä¸­å¤®ç€åœ°ã‚’å¼·ãä¿ƒã™ï¼‰
                shaped_reward -= abs(x) * self.landing_penalty_weight
            else:
                # ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ç­‰ã®å¤±æ•—ã¯ãã®ã¾ã¾ï¼ˆè² ã®å ±é…¬ã«ä½ç½®ç½°ã‚’è¿½åŠ ã—ã¦ã‚‚è‰¯ã„ãŒã“ã“ã§ã¯ä¸å¤‰ï¼‰
                pass

        return obs, shaped_reward, terminated, truncated, info

    def reset(self, **kwargs):
        return self.env.reset(**kwargs)

# -------------------------
# ç’°å¢ƒä½œæˆé–¢æ•°ï¼ˆPPOå­¦ç¿’ç”¨ï¼‰
# -------------------------
def make_train_env():
    # ãƒ™ãƒ¼ã‚¹ç’°å¢ƒï¼ˆGymnasium ã® LunarLander-v3ï¼‰
    base_env = gym.make("LunarLander-v3")
    # ã‚«ã‚¹ã‚¿ãƒ å ±é…¬ãƒ©ãƒƒãƒ‘ãƒ¼ã‚’é©ç”¨
    wrapped = CenterRewardWrapper(base_env, step_penalty_weight=2.0, landing_penalty_weight=50.0)
    return wrapped

# DummyVecEnv ç”¨ãƒ©ãƒƒãƒ‘ãƒ¼é–¢æ•°ï¼ˆSB3ç”¨ï¼‰
def make_env_for_vec():
    return make_train_env()

# -------------------------
# PPO ã¨å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
# -------------------------
# å­¦ç¿’ç’°å¢ƒï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰ -- 1ã‚³ã‚¢å­¦ç¿’ï¼ˆå¿…è¦ãªã‚‰è¤‡æ•°ä½œã‚‹ï¼‰
env = DummyVecEnv([make_env_for_vec])

# PPO ãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆå¿…è¦ãªã‚‰ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã“ã“ã§å¤‰æ›´ï¼‰
model = PPO(
    policy="MlpPolicy",
    env=env,
    verbose=1,
    learning_rate=3e-4,    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã€‚å¿…è¦ã«å¿œã˜ã¦å¤‰æ›´
    n_steps=2048,
    batch_size=64,
    n_epochs=10,
    gamma=0.99,
)

# -------------------------
# å­¦ç¿’å®Ÿè¡Œ
# -------------------------
TOTAL_TIMESTEPS = 400_000   # å­¦ç¿’é‡ã€‚ä¸­å¤®ç€é™¸ã‚’å­¦ã°ã›ã‚‹ãŸã‚ã«å¢—ã‚„ã—ã¦ã‚ã‚‹
print("å­¦ç¿’é–‹å§‹:", time.ctime())
model.learn(total_timesteps=TOTAL_TIMESTEPS)
print("å­¦ç¿’çµ‚äº†:", time.ctime())

# ãƒ¢ãƒ‡ãƒ«ä¿å­˜
model.save("ppo_lunarlander_centerreward")

# -------------------------
# è©•ä¾¡ â†’ å‹•ç”»ç”Ÿæˆ
# -------------------------
video_folder = "./videos"
os.makedirs(video_folder, exist_ok=True)

# è©•ä¾¡ç”¨ç’°å¢ƒã¯ render_mode ã‚’ä½¿ã„ã€RecordVideo ã§ä¿å­˜ï¼ˆãƒ©ãƒƒãƒ‘ãƒ¼ã¯è©•ä¾¡æ™‚ã«ä¸è¦ï¼‰
eval_env = gym.make("LunarLander-v3", render_mode="rgb_array")
# RecordVideo ã‚’ä½¿ã†ã¨è‡ªå‹•ã§ mp4 ãŒç”Ÿæˆã•ã‚Œã‚‹
eval_env = RecordVideo(eval_env, video_folder=video_folder, name_prefix="ppo_center_lander")

# è©•ä¾¡ï¼ˆå‹•ç”»ã«ä¿å­˜ã™ã‚‹ã‚¹ãƒ†ãƒƒãƒ—ï¼‰
obs, info = eval_env.reset()
for _ in range(1000):
    action, _ = model.predict(obs, deterministic=True)
    obs, reward, terminated, truncated, info = eval_env.step(action)
    if terminated or truncated:
        break

# RecordVideo ã‚’ç¢ºå®Ÿã«é–‰ã˜ã‚‹
eval_env.close()

# -------------------------
# ç”Ÿæˆã•ã‚ŒãŸå‹•ç”»ã‚’è‡ªå‹•æ¤œå‡ºã—ã¦å†ç”Ÿ
# -------------------------
mp4_files = [f for f in os.listdir(video_folder) if f.endswith(".mp4")]
mp4_files.sort(key=lambda x: os.path.getmtime(os.path.join(video_folder, x)))

if len(mp4_files) == 0:
    print("âŒ å‹•ç”»ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
else:
    latest = os.path.join(video_folder, mp4_files[-1])
    print("ğŸ¥ æœ€æ–°å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«:", latest)
    display(Video(latest, embed=True))