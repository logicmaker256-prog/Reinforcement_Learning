強化学習シナリオ

このシナリオは、強化学習の勉強の為、ブロック崩しを模した環境での学習に関するものである。
学習は、カリキュラム学習を用い、学習負荷を低減します。
負荷変動は、コートのサイズ、ブロックの数によって行う。

■座標系・時間軸

原点：左下奥を (0, 0) とする

X軸：右がプラス

Y軸：上がプラス

単位：ピクセル

1ステップ（1st）＝ 1msec

✅ 観測（Observation）

エージェントには以下を渡す：

ラケット位置 x

ボール位置 (x, y)

ボール速度 (Vx, Vy)

存在するブロックの座標リスト
　（例：最大16個 ⇒ 16×2 の位置ベクトル）

ブロックの生存フラグ（1 or 0）

【移動体】

■ラケット×1台：
・ＡＩ学習の対象
・ボールに当たる事でボールを打ったと
　判定され、ボールが跳ね返る
・周囲にLaserを出しており以下を把握
　・自分の位置
　・ボールの位置
　・ブロックの位置

・オブジェクトは、縦10横50の四角とする

・ラケットの動きと速度は以下の様になる
　・右に少し進む　　2ピクセル/st
　・右に大きく進む　6ピクセル/st
　・左に少し進む　　-2ピクセル/st
　・左に大きく進む　-6ピクセル/st
　・止まる
　※行動は1ステップごとに変更可能

・ラケットの状態
　・位置：横位置x
　　(縦位置yは、固定値10とする
　・初期位置は、縦10横75

・ラケットの横方向幅を 0〜100% とし、
　ボールが当たる位置で反射角を変える
　・ラケットの0%以上〜25%未満に
　　当たると　反射角+20度
　・ラケットの25%以上〜75%未満に
　　当たると反射角を変えない
　・ラケットの75%以上〜100%以下に
　　当たると　反射角-20度

■ボール×1個
・ラケットに当たることで反射する
・オブジェクトは、直径10の円で
　歪まない
・ボールの状態
　・位置：x,y
　・速度：Vx,Vy
　　(ベクトル和は3ピクセル/st)
・初期位置は、ラケット中央に
　乗っかっている。ゲーム開始と同時に
　コートへとんでいく。

・ボールを落とすと報酬 -10 を与えて
　エピソード終了
　ラケット、ボール、ブロックを初期化
　して次のエピソード開始

・ボールの挙動
　壁・ラケット・ブロックと衝突すると
　反射する
　底面から落ちた場合：エピソード
　終了（Game Over）


【構造物】

■コート
・コートの下端以外に壁があり、
　ボールは反射する
・下端は床ではなく
　落下エリア（Ball Out）

・コートの幅はカリキュラム学習で
　段階的に変化
　１番目　左下(50,0)―右上(170,200)
　２番目　左下(25,0)―右上(195,200)
　３番目　左下(0,0)―右上(220,200)

・外周ラインに触れたボールは、
　反射してコートに戻る。
・外周ラインの横位置は、学習の進み方で
　変わる

■ブロック
・オブジェクトは、縦8×横48
　(縦10横50ステップで扱うと隙間が
　作れます)
・学習の進み方で数が変わる。

１番目
　数　　４個
　場所　左下(60,150)―右上(109,159)
　　　　左下(110,150)―右上(159,159)
　　　　左下(60,160)―右上(109,169)
　　　　左下(110,160)―右上(159,169)

２番目
　数　　９個
　場所　左下(35,150)―右上(84,159)
　　　　左下(85,150)―右上(134,159)
　　　　左下(135,150)―右上(184,159)
　　　　左下(35,160)―右上(84,169)
　　　　左下(85,160)―右上(134,169)
　　　　左下(135,160)―右上(184,169)
　　　　左下(35,170)―右上(84,179)
　　　　左下(85,170)―右上(134,179)
　　　　左下(135,170)―右上(184,179)

３番目
　数　　１６個
　場所　左下(10,150)―右上(59,159)
　　　　左下(60,150)―右上(109,159)
　　　　左下(110,150)―右上(159,159)
　　　　左下(160,150)―右上(209,159)
　　　　左下(10,160)―右上(59,169)
　　　　左下(60,160)―右上(109,169)
　　　　左下(110,160)―右上(159,169)
　　　　左下(160,160)―右上(209,169)
　　　　左下(10,170)―右上(59,179)
　　　　左下(60,170)―右上(109,179)
　　　　左下(110,170)―右上(159,179)
　　　　左下(160,170)―右上(209,179)
　　　　左下(10,180)―右上(59,189)
　　　　左下(60,180)―右上(109,189)
　　　　左下(110,180)―右上(159,189)
　　　　左下(160,180)―右上(209,189)


・縦ブロックに当たったら水平方向反射
・横ブロックに当たったら垂直方向反射

【報酬設計】

・ブロックを消す　　　+1
・ブロックを全て消す　+20
・ラケットが５回ボールに当たる　+20
・ボールを落とす　　　-1
・ボールの水平方向と同じ方向にうごく　+0.2/st
・ボールの水平方向と反対方向にうごく　-0.2/st

【その他】

■GoogleColabで動作可
■学習後に学習結果を示す
　・エボックとトータルリワードのグラフ
　・学習結果を示す動画を再生
■εが0.97以下のε-greedyとする
■ラケットが５回ボールに当たる。
　又は、ブロックを全て消すで「成功」
　とする。
■直近の10回で成功の比率が80%を
　超えたら、
①今のレベルの動画を作成する
②カリキュラム学習を+1進める。
　(3回目で最終である)
■成功比率はエピソード10以降で表示
■学習中の表示
　エピソード,リワード,直近10回の成功比率(移動平均値),レベル

■エピソードの終了条件を示す。
①ボール落下 → Game Over
②ブロック全破壊 → Clear
③最大ステップ数到達（5000 step ）
④ラケットが５回ボールに当たる

**【強化学習アルゴリズム（PPO）】
本環境の学習には PPO（Proximal Policy Optimization）を使用する。
方策ネットワークと価値ネットワークは共有（Actor-Critic方式）とする。
PPO-Clip 損失（clip range = 0.2）を用いて更新し、GAE(λ=0.95) を利用する。
ハイパーパラメータは以下の通り：

学習率 3e-4

割引率 γ = 0.99

GAE λ = 0.95

エポック数 = 4

ミニバッチサイズ = 2048

Advantage を正規化

Entropy bonus = 0.01

Value loss coef = 0.5

ネットワーク構造：MLP 2層（256, 256）
以上の条件で、ラケットの操作方策 π を改善していく。**


