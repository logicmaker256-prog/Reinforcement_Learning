# === DoorKey 用 PPO + ICM (CNN共有) — Colab-ready ===
# 1セルに丸ごと貼って実行してOK
!pip install -q minigrid gymnasium moviepy pillow

import gymnasium as gym
import minigrid
from minigrid.wrappers import RGBImgPartialObsWrapper, ImgObsWrapper
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from PIL import Image
from moviepy.editor import ImageSequenceClip
from IPython.display import Video, display
import os

# --------------------------------------------------
# Settings
# --------------------------------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("device:", device)

ENV_ID = "MiniGrid-DoorKey-5x5-v0"   # DoorKey 5x5
RENDER_MODE = "rgb_array"
FEATURE_DIM = 128
ICM_SCALE = 0.12      # weight for icm loss in total loss
LEARNING_RATE = 3e-4
EPISODES = 1000       # 増やせば性能向上
STEPS_PER_EP = 128
GAMMA = 0.99
LAMBDA = 0.95

# --------------------------------------------------
# Helpers: force resize to 7x7 (MiniGrid partial obs is 7x7 but just in case)
# --------------------------------------------------
def to_7x7(img):
    # img: HxWx3 numpy uint8
    pil = Image.fromarray(img)
    pil2 = pil.resize((7,7), resample=Image.BILINEAR)
    arr = np.array(pil2)  # (7,7,3) uint8
    return arr

# --------------------------------------------------
# Networks: shared CNN encoder, policy head, value head, ICM
# --------------------------------------------------
class CNNEncoder(nn.Module):
    def __init__(self, out_dim=FEATURE_DIM):
        super().__init__()
        # input expected: (B,3,7,7)
        self.cnn = nn.Sequential(
            nn.Conv2d(3, 32, 3, stride=1, padding=1), nn.ReLU(),
            nn.Conv2d(32, 64, 3, stride=1, padding=1), nn.ReLU(),
            nn.Flatten()
        )
        self.fc = nn.Linear(64 * 7 * 7, out_dim)

    def forward(self, x):
        # x: tensor (B,3,7,7)
        h = self.cnn(x)
        return self.fc(h)  # (B, out_dim)

class PolicyValue(nn.Module):
    def __init__(self, feature_dim, action_dim):
        super().__init__()
        self.actor = nn.Linear(feature_dim, action_dim)
        self.critic = nn.Linear(feature_dim, 1)
    def forward(self, phi):
        logits = self.actor(phi)
        value = self.critic(phi)
        return logits, value.squeeze(-1)

class ICM(nn.Module):
    def __init__(self, feat_dim, action_dim):
        super().__init__()
        self.inv = nn.Sequential(
            nn.Linear(feat_dim * 2, 256), nn.ReLU(),
            nn.Linear(256, action_dim)
        )
        self.fwd = nn.Sequential(
            nn.Linear(feat_dim + action_dim, 256), nn.ReLU(),
            nn.Linear(256, feat_dim)
        )
    def forward(self, phi_s, phi_ns, a_onehot):
        # phi_s, phi_ns: (B, feat_dim)
        inv_in = torch.cat([phi_s, phi_ns], dim=1)
        pred_a = self.inv(inv_in)  # logits over actions
        fwd_in = torch.cat([phi_s, a_onehot], dim=1)
        pred_phi_ns = self.fwd(fwd_in)
        intrinsic = 0.5 * (pred_phi_ns - phi_ns).pow(2).sum(dim=1)  # (B,)
        return intrinsic, pred_a, pred_phi_ns

# --------------------------------------------------
# Agent wrapper (holds networks and optimizer)
# --------------------------------------------------
class PPO_ICM_Agent:
    def __init__(self, action_dim, feature_dim=FEATURE_DIM):
        self.encoder = CNNEncoder(feature_dim).to(device)
        self.pv = PolicyValue(feature_dim, action_dim).to(device)
        self.icm = ICM(feature_dim, action_dim).to(device)
        self.opt = optim.Adam(
            list(self.encoder.parameters()) + list(self.pv.parameters()) + list(self.icm.parameters()),
            lr=LEARNING_RATE
        )
        self.action_dim = action_dim

    def act(self, obs_np):
        # obs_np: (7,7,3) uint8
        x = torch.tensor(obs_np.transpose(2,0,1), dtype=torch.float32, device=device).unsqueeze(0) / 255.0
        phi = self.encoder(x)               # (1,feat)
        logits, value = self.pv(phi)        # (1,act), (1,)
        dist = torch.distributions.Categorical(logits=logits)
        a = dist.sample().item()
        logp = dist.log_prob(torch.tensor(a, device=device)).item()
        return a, logp, value.item(), phi.detach()

    def update(self, batch):
        # batch elements are numpy arrays or tensors already converted below
        obs, next_obs, actions, advs, returns, old_logp = batch
        # convert to tensors
        obs_t = torch.tensor(obs.transpose(0,3,1,2), dtype=torch.float32, device=device) / 255.0  # (B,3,7,7)
        next_obs_t = torch.tensor(next_obs.transpose(0,3,1,2), dtype=torch.float32, device=device) / 255.0
        actions_t = torch.tensor(actions, dtype=torch.long, device=device)
        advs_t = torch.tensor(advs, dtype=torch.float32, device=device)
        returns_t = torch.tensor(returns, dtype=torch.float32, device=device)
        old_logp_t = torch.tensor(old_logp, dtype=torch.float32, device=device)

        # forward pass
        phi = self.encoder(obs_t)       # (B,feat)
        phi_ns = self.encoder(next_obs_t)

        # policy/value
        logits, values = self.pv(phi)
        dist = torch.distributions.Categorical(logits=logits)
        logp = dist.log_prob(actions_t)
        ratio = torch.exp(logp - old_logp_t)
        # PPO clipped objective
        clip_eps = 0.2
        surr1 = ratio * advs_t
        surr2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * advs_t
        policy_loss = -torch.min(surr1, surr2).mean()
        value_loss = nn.MSELoss()(values, returns_t)

        # ICM losses
        a_onehot = torch.nn.functional.one_hot(actions_t, num_classes=self.action_dim).float()
        intrinsic, pred_a, pred_phi_ns = self.icm(phi, phi_ns, a_onehot)
        inv_loss = nn.CrossEntropyLoss()(pred_a, actions_t)
        fwd_loss = nn.MSELoss()(pred_phi_ns, phi_ns.detach())
        icm_loss = inv_loss + fwd_loss

        # total loss
        loss = policy_loss + 0.5 * value_loss + ICM_SCALE * icm_loss

        self.opt.zero_grad()
        loss.backward()
        self.opt.step()

        return {
            "loss": loss.item(),
            "policy_loss": policy_loss.item(),
            "value_loss": value_loss.item(),
            "icm_loss": icm_loss.item(),
            "intrinsic_mean": intrinsic.mean().item()
        }

# --------------------------------------------------
# Environment constructor (render_mode must be rgb_array)
# --------------------------------------------------
def make_env():
    env = gym.make(ENV_ID, render_mode=RENDER_MODE)
    # use partial image observation (7x7) and ImgObsWrapper to return ndarray
    env = RGBImgPartialObsWrapper(env)  # gives obs["image"]=HxWx3
    env = ImgObsWrapper(env)            # obs -> ndarray HxWx3
    return env

env = make_env()
action_dim = env.action_space.n
agent = PPO_ICM_Agent(action_dim)

# --------------------------------------------------
# Training loop (on-policy, per-episode collection)
# --------------------------------------------------
save_log = []
for ep in range(1, EPISODES + 1):
    obs, _ = env.reset()
    obs = to_7x7(obs)
    ep_reward = 0.0

    buf_obs = []
    buf_next_obs = []
    buf_actions = []
    buf_rewards = []
    buf_logp = []
    buf_values = []
    buf_dones = []

    for step in range(STEPS_PER_EP):
        a, logp, value, phi = agent.act(obs)
        next_obs, reward, terminated, truncated, info = env.step(a)
        next_obs = to_7x7(next_obs)

        buf_obs.append(obs.copy())
        buf_next_obs.append(next_obs.copy())
        buf_actions.append(a)
        buf_rewards.append(reward)
        buf_logp.append(logp)
        buf_values.append(value)
        buf_dones.append(1.0 if (terminated or truncated) else 0.0)

        obs = next_obs
        ep_reward += reward
        if terminated or truncated:
            break

    # bootstrap value for last state
    last_value = 0.0
    if not (terminated or truncated):
        # estimate last value
        _, _, last_value, _ = agent.act(obs)

    # compute GAE advantages and returns
    values = buf_values + [last_value]
    advantages = []
    gae = 0.0
    for t in reversed(range(len(buf_rewards))):
        delta = buf_rewards[t] + GAMMA * values[t+1] * (1 - buf_dones[t]) - values[t]
        gae = delta + GAMMA * LAMBDA * (1 - buf_dones[t]) * gae
        advantages.insert(0, gae)
    returns = [advantages[i] + values[i] for i in range(len(advantages))]

    if len(buf_obs) > 0:
        batch = (
            np.array(buf_obs, dtype=np.uint8),
            np.array(buf_next_obs, dtype=np.uint8),
            np.array(buf_actions, dtype=np.int64),
            np.array(advantages, dtype=np.float32),
            np.array(returns, dtype=np.float32),
            np.array(buf_logp, dtype=np.float32)
        )
        stats = agent.update(batch)
    else:
        stats = {"loss":0.0, "intrinsic_mean":0.0}

    save_log.append((ep, ep_reward, stats["loss"], stats["intrinsic_mean"]))
    if ep % 10 == 0 or ep == 1:
        print(f"EP {ep:04d} reward={ep_reward:.2f} loss={stats['loss']:.4f} intrinsic={stats['intrinsic_mean']:.6f}")

# --------------------------------------------------
# Save model (optional)
# --------------------------------------------------
os.makedirs("models", exist_ok=True)
torch.save({
    "encoder": agent.encoder.state_dict(),
    "pv": agent.pv.state_dict(),
    "icm": agent.icm.state_dict()
}, "models/ppo_icm_doorkey.pth")
print("Model saved.")

# --------------------------------------------------
# Video: PPO agent vs Random (side-by-side)
# --------------------------------------------------
def record_episode_frames(env, agent=None, max_steps=300):
    frames = []
    obs, _ = env.reset()
    obs = to_7x7(obs)
    for _ in range(max_steps):
        if agent is None:
            a = env.action_space.sample()
        else:
            a, _, _, _ = agent.act(obs)
        next_obs, reward, terminated, truncated, info = env.step(a)
        # use render() to get rgb array
        frame = env.render()
        if frame is None:
            # fallback: use the resized observation as rgb
            frame = to_7x7(next_obs)
        frames.append(frame)
        obs = to_7x7(next_obs)
        if terminated or truncated:
            break
    return frames

print("Recording PPO agent (video)...")
ppo_frames = record_episode_frames(env, agent)
print("Recording Random agent (video)...")
rand_frames = record_episode_frames(env, None)

# equalize lengths
L = min(len(ppo_frames), len(rand_frames))
ppo_frames = ppo_frames[:L]
rand_frames = rand_frames[:L]

# combine side-by-side
combined = [np.concatenate([f1, f2], axis=1) for f1,f2 in zip(ppo_frames, rand_frames)]

out_file = "ppo_vs_random_doorkey.mp4"
print("Saving video to", out_file)
clip = ImageSequenceClip([frame.astype(np.uint8) for frame in combined], fps=6)
clip.write_videofile(out_file, codec="libx264", verbose=False, logger=None)

display(Video(out_file, embed=True))